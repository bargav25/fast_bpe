📖 Reading and splitting corpus from data/TinyStoriesV2-GPT4-train.txt...
🧩 Split into 2717700 chunks.
⚙️ Tokenizing with 16 workers...
💾 Writing 532299551 tokens to output/train_ids.memmap as memmap...
✅ Done. Config for training:
    dtype: int32
    data_shape: [532299551]
